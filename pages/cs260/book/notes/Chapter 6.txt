6.1 Objectives

To be able to explain and implement sequential search and binary search.
To be able to explain and implement selection sort, bubble sort, merge sort, quick sort, insertion sort, and shell sort.
To understand the idea of hashing as a search technique.
To introduce the map abstract data type.
To implement the map abstract data type using hashing.

6.3 Sequential search

This search just scans the collection from the lowest to highest index, comparing the results on the way to see if there's a match.

A good distinction to note with the sequential search is that it really works best with ordered lists. 
With an ordered list, if we have a key that isn't in the data collection then we can stop once we've compared a value that's larger than the key we're looking for.

Here's the comparisons of the sequential search, ordered by case, best case, worst case, and average case.

item is present : 1 : n : n/2
item is not present : n : n : n

Of course, as n gets inifinitely large, n/2 turns into n. So we have a big O of n for the sequential search.

6.4 Binary search

We can get more efficiency with an ordered list if we use a binary search.

The way it works with a binary search is:
    1. We start with our ordered list
    2. We compare the middle item of the list, If we find our item, then we end the search. If we can't find it and it is greater than the middle of the list, then we discard the first half of the list and repeat step 2.

The binary search works well because it divides the program up into smaller pieces.

As such, the binary search has a big O of log(n).

6.5 hashing

Hashing is very efficient. I'm going to go over how it works.

A hash table is a collection of items that makes it easy to find them later. Each position, called a slot, can hold an item and is indexed starting at 0. The mapping between an item and the slot where that item belongs is called a hash function. The hash function will take any item in the collection and return an integer in the range of slot names, between 0 and m-1.

Once the hash values have been computed, we can insert each item into the hash table at the designated position that we calculated. The integer we calculated is where they will be indexed at.
The occupancy among this is called the load factor, and is denoted by dividing the number of items by the table size. 

So now if we want to search for something, we just put it into our handy dandy hash function and we can immediately get the index. This operation is O(1) and super efficient.

But what happens if one or more items have the same spot? This can happen, and it's called a collision. Both objects are now assigned to one index, and it makes the hashing algorithm undesirable if it producted lots of collisions.

A hash function that maps every single item to a slot is called a perfect hashing function. The goal when creating a hashing function is to reduce collisions, to be fast, to create an even distribution of assignments, and to reduce memory.

The folding method for construction hash funtions is as such: we reduce the input into equal size pieces, add them together, then divide by the amount of slots that you have and keep the remainder. So for example, 541-867-5309 would be broken up into 54,18,67,53,09 , then added up (54+18+67+53+09) = 201, then divided by our slots and keeping the remainder. (201 % 11 ) = 6, so that would go in index 6.

Another example of this is the mid square method. We square the input, then take the inside digits, then divide them with the amount of slots and keep the remainder. For examaple, if our item were 44, we would square it to be 1,936, keep the middle two numbers (93), then divide it and keep the remainder (93 % 11) = 5.

To hash a string, we can think of each letter as a number and then compute it that way.

Resolving collisions: collisions happen, and they need to be dealt with if we implement hashing algorithms. One way we can do this is by simply adding the value to the next available slot. This is called open addressing. So we just implement a step in our hashing algorithm to look for slots to the right of ours if the value isn't immediately caught, this is called linear probing. A disadvantage of linear probing is the tendency for clustering, large amounts of collisions happening in one area, creating an unbalanced hash. This can be resolved by instead of automatically filling slots next to the collisioned index, jumping a couple of indexes. This is called rehashing.

A variation of linear probing is called quadratic probing. Instead of spacing out collisions linearly with a fixed number, a quadratic probing uses a skip consisting of successive perfect squares. This helps to flatten clusters.

Another way of handling clusters is chaining. This just avoids the problem by allowing multiple values to inhabit one slot at the same time.

Implementation of the map abstract data type

Look it up on runestone.

6.7 Bubble sort

The bubble sort makes multiple passes through a list, comparing two indexes and swapping them.
After the first pass, the largest number is at the end of the list. After every subsequent pass, the larger numbers are pushed to the end of the list.
After completing n-1 passes, the list is sorted.

In python, we can do the "swap" in one step.

traditional:

temp = alist[i]
alist[i] = alist[j]
alist[j] = temp

python:

a,b=b,a

Bubble sort is best case O(n), worst case O(n^2), and average case O(n^2)

Bubble sort implementation

def bubbleSort(alist):
    for passnum in range(len(alist)-1,0,-1):
        for i in range(passnum):
            if alist[i]>alist[i+1]:
                temp = alist[i]
                alist[i] = alist[i+1]
                alist[i+1] = temp

alist = [54,26,93,17,77,31,44,55,20]
bubbleSort(alist)
print(alist)

6.8 Selection sort

The Selection sort improves upon the bubble sort. Instead of swapping every single index, the selection sort just looks for the largest value and then moves it to the end of the list. After the first pass, the first item is in the right place, after the second pass likewise, so on and so forth.

Here's an example of selectionSort implementation.

def selectionSort(alist):
   for fillslot in range(len(alist)-1,0,-1):
       positionOfMax=0
       for location in range(1,fillslot+1):
           if alist[location]>alist[positionOfMax]:
               positionOfMax = location

       temp = alist[fillslot]
       alist[fillslot] = alist[positionOfMax]
       alist[positionOfMax] = temp

alist = [54,26,93,17,77,31,44,55,20]
selectionSort(alist)
print(alist)

Bubble sort is still technically O(n^2), but performs better in real life tests.

6.9 The insertion sort

The intertion sort is still O(n^2), but it works differently.

It works by maintaining a sorted sublist in lower portions of the list, and then inserting the values in order to make the sublist grow larger.

example implementation:

def insertionSort(alist):
   for index in range(1,len(alist)):

     currentvalue = alist[index]
     position = index

     while position>0 and alist[position-1]>currentvalue:
         alist[position]=alist[position-1]
         position = position-1

     alist[position]=currentvalue

alist = [54,26,93,17,77,31,44,55,20]
insertionSort(alist)
print(alist)

6.10 The shell sort

The shell sort is an improved insertion sort, with multiple sub lists. Then, once we've sorted the sub lists, our values are closer to where they need to be.
The shell sort uses i, sometimes called a gap, for how long our sublists are going to be. After we've sorted the sub-sublists, we do an insertion sort over the entire list.
The thing to notice with the gap, though, is that it's spaced kind of weird.
For example: if we subdivide the following list with i=3, the lists would be:
[1,2,3,4,5,6,7,8,9,10] 
1: [1,4,8]
2: [2,5,9]
3: [3,6,10]


example implementation:

def shellSort(alist):
    sublistcount = len(alist)//2
    while sublistcount > 0:

      for startposition in range(sublistcount):
        gapInsertionSort(alist,startposition,sublistcount)

      print("After increments of size",sublistcount,
                                   "The list is",alist)

      sublistcount = sublistcount // 2

def gapInsertionSort(alist,start,gap):
    for i in range(start+gap,len(alist),gap):

        currentvalue = alist[i]
        position = i

        while position>=gap and alist[position-gap]>currentvalue:
            alist[position]=alist[position-gap]
            position = position-gap

        alist[position]=currentvalue

alist = [54,26,93,17,77,31,44,55,20]
shellSort(alist)
print(alist)

The big O of a shell sort is beyond the scope of this text, but falls between O(n) and O(n^2)

ex:
 
[5, 16, 20, 12, 3, 8, 9, 17, 19, 7]
1: [5,12,9,7]
2: [16,3,17]
3: [20,8,19]
after sorting:
1: [5,7,9,12]
2: [3,16,17]
4: [8,19,20]
after insertion, before insertion sort:
[5,3,8,7,16,19,9,17,20,12]

6.11 The Merge Sort

The merge sort breaks apart a list into individual pieces, then merging them into smaller sorted sublists and then combinging those sublists to create the final desired sorted list.

Here's a sample implementation:

def mergeSort(alist):
    print("Splitting ",alist)
    if len(alist)>1:
        mid = len(alist)//2
        lefthalf = alist[:mid]
        righthalf = alist[mid:]

        mergeSort(lefthalf)
        mergeSort(righthalf)

        i=0
        j=0
        k=0
        while i < len(lefthalf) and j < len(righthalf):
            if lefthalf[i] <= righthalf[j]:
                alist[k]=lefthalf[i]
                i=i+1
            else:
                alist[k]=righthalf[j]
                j=j+1
            k=k+1

        while i < len(lefthalf):
            alist[k]=lefthalf[i]
            i=i+1
            k=k+1

        while j < len(righthalf):
            alist[k]=righthalf[j]
            j=j+1
            k=k+1
    print("Merging ",alist)

alist = [54,26,93,17,77,31,44,55,20]
mergeSort(alist)
print(alist)

The merge sort is O(nlogn).

6.12 The quick sort

Quick sort works like a lot of other algorithms on this list, by divide and conquer. It starts by picking a pivot point arbitrarily, usually 1, and then uses it to assist in splitting the list.

After we select the pivot point, we locate two position markers. We then begin moving values around so they are on the right side of the pivot point. They will converge on the split point.

We begin by incrementing leftmark until we locate a value that is greater than the pivot value. We then decrement rightmark until we find a value that is less than the pivot value. At this point we have discovered two items that are out of place with respect to the eventual split point. For our example, this occurs at 93 and 20. Now we can exchange these two items and then repeat the process again.

At the point where rightmark becomes less than leftmark, we stop. The position of rightmark is now the split point. The pivot value can be exchanged with the contents of the split point and the pivot value is now in place (Figure 14). In addition, all the items to the left of the split point are less than the pivot value, and all the items to the right of the split point are greater than the pivot value. The list can now be divided at the split point and the quick sort can be invoked recursively on the two halves.

