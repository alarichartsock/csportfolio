    7.1 Objectives

To understand what a tree data structure is and how it is used.
To see how trees can be used to implement a map data structure.
To implement trees using a list.
To implement trees using classes and references.
To implement trees as a recursive data structure.
To implement a priority queue using a heap.

    7.2 Examples of trees

What is a tree?

A tree data structure has a root, branches, and leaves. The difference between a tree in nature and a tree in computer science is that a tree data structure has its root at the top and its leaves on the bottom.

A second property of trees is that all of the children of one node are independent of the children of another node.

A third property is that each leaf node is unique. 

Another important property of trees, derived from their hierarchical nature, is that you can move entire sections of a tree (called a subtree) to a different position in the tree without affecting the lower levels of the hierarchy.

    7.3 Vocabulary and Definitions

Node : A node is a fundamental part of a tree. It can have a name, which we call the “key.” A node may also have additional information. We call this additional information the “payload.” While the payload information is not central to many tree algorithms, it is often critical in applications that make use of trees.

Edge : An edge is another fundamental part of a tree. An edge connects two nodes to show that there is a relationship between them. Every node (except the root) is connected by exactly one incoming edge from another node. Each node may have several outgoing edges.

Root : The root of the tree is the only node in the tree that has no incoming edges. In Figure Figure 2, / is the root of the tree.

Path : A path is an ordered list of nodes that are connected by edges. For example, Mammal -> Carnivora → Felidae → Felis → Domestica is a path.

Children : The set of nodes c that have incoming edges from the same node to are said to be the children of that node. In Figure Figure 2, nodes log/, spool/, and yp/ are the children of node var/.

Parent : A node is the parent of all the nodes it connects to with outgoing edges. In Figure 2 the node var/ is the parent of nodes log/, spool/, and yp/.

Sibling : Nodes in the tree that are children of the same parent are said to be siblings. The nodes etc/ and usr/ are siblings in the filesystem tree.

Subtree : A subtree is a set of nodes and edges comprised of a parent and all the descendants of that parent.

Leaf Node : A leaf node is a node that has no children. For example, Human and Chimpanzee are leaf nodes in Figure 1.

Level : The level of a node n
is the number of edges on the path from the root node to n. For example, the level of the Felis node in Figure 1 is five. By definition, the level of the root node is zero.

Height : The height of a tree is equal to the maximum level of any node in the tree. The height of the tree in Figure 2 is two.

Formal definition of a tree:

Definition One: A tree consists of a set of nodes and a set of edges that connect pairs of nodes. A tree has the following properties:
    One node of the tree is designated as the root node.
    Every node n, except the root node, is connected by an edge from exactly one other node p, where p is the parent of n
    A unique path traverses from the root to each node.
    If each node in the tree has a maximum of two children, we say that the tree is a binary tree.

Definition Two: A tree is either empty or consists of a root and zero or more subtrees, each of which is also a tree.

7.4 List of Lists Representation

We can use nested lists to represent tree in Python.

We do this by inserting our head at [0], left subtree at [1], and right subtree at [2]. Then rinse and repeat for future sublists.

We take this pattern and we can create a Tree class.

def BinaryTree(r):
    return [r, [], []]

def insertLeft(root,newBranch):
    t = root.pop(1)
    if len(t) > 1:
        root.insert(1,[newBranch,t,[]])
    else:
        root.insert(1,[newBranch, [], []])
    return root

def insertRight(root,newBranch):
    t = root.pop(2)
    if len(t) > 1:
        root.insert(2,[newBranch,[],t])
    else:
        root.insert(2,[newBranch,[],[]])
    return root

def getRootVal(root):
    return root[0]

def setRootVal(root,newVal):
    root[0] = newVal

def getLeftChild(root):
    return root[1]

def getRightChild(root):
    return root[2]

    7.5 Nodes and references

We can also represent a binary tree with nodes and references, instead of nested lists.

Here's an implementation:

class BinaryTree:
    def __init__(self,rootObj):
        self.key = rootObj
        self.leftChild = None
        self.rightChild = None

    def insertLeft(self,newNode):
        if self.leftChild == None:
            self.leftChild = BinaryTree(newNode)
        else:
            t = BinaryTree(newNode)
            t.leftChild = self.leftChild
            self.leftChild = t

    def insertRight(self,newNode):
        if self.rightChild == None:
            self.rightChild = BinaryTree(newNode)
        else:
            t = BinaryTree(newNode)
            t.rightChild = self.rightChild
            self.rightChild = t


    def getRightChild(self):
        return self.rightChild

    def getLeftChild(self):
        return self.leftChild

    def setRootVal(self,obj):
        self.key = obj

    def getRootVal(self):
        return self.key

    7.6 Parse tree

Parse trees can be used to represent real world problems. That's all I'm going to be saying in this section.

    7.7 Tree Traversal

We can visit a tree in three ways. Preorder, inorder, and postorder.

preorder :  In a preorder traversal, we visit the root node first, then recursively do a preorder traversal of the left subtree, followed by a recursive preorder traversal of the right subtree.
inorder : In an inorder traversal, we recursively do an inorder traversal on the left subtree, visit the root node, and finally do a recursive inorder traversal of the right subtree.
postorder : In a postorder traversal, we recursively do a postorder traversal of the left subtree and the right subtree followed by a visit to the root node.

Preorder: (Root, Left, Right)
Inorder: (Left, Root, Right)
Postorder: (Left, Right, Root)

Here's some implementations:

def preorder(tree):
    if tree:
        print(tree.getRootVal())
        preorder(tree.getLeftChild())
        preorder(tree.getRightChild())

def postorder(tree):
    if tree != None:
        postorder(tree.getLeftChild())
        postorder(tree.getRightChild())
        print(tree.getRootVal())

def inorder(tree):
  if tree != None:
      inorder(tree.getLeftChild())
      print(tree.getRootVal())
      inorder(tree.getRightChild())

    7.8 Priory Queues with Binary Heaps

A priority queue acts like a regular queue, but it assigns a priority to items enqueued. These items get to jump to the front based on their priority.

The most popular way to implement a priority queue is to implement a binary heap. We can enqueue and dequeue items with big log(n).. sweet!

The binary heap has two common variants: the min heap, which the smallest key is in the back, and the max heap, in which the largest key is always up front.

    7.10 Binary Heap implementation

To start, we need a complete binary tree. This is a tree that has all of its nodes.

The method that we will use to store items in a heap relies on maintaining the heap order property. 
The heap order property is as follows: In a heap, for every node x with parent p, the key in p is smaller than or equal to the key in x.

Alright, so let's think about our other operations. To add an item too the heap, we could just append it but this will almost certainly break our heap strucutre. So instead we append it, and then "percolate" it up. (Think bubble it up to the root of the tree). This involves swapping it until it's in it's correct heap order.

def percUp(self,i):
    while i // 2 > 0:
      if self.heapList[i] < self.heapList[i // 2]:
         tmp = self.heapList[i // 2]
         self.heapList[i // 2] = self.heapList[i]
         self.heapList[i] = tmp
      i = i // 2

def insert(self,k):
    self.heapList.append(k)
    self.currentSize = self.currentSize + 1
    self.percUp(self.currentSize)

Now that we've defined insert, let's go over delMin. If we want to delete the minimum value in our tree, finding this value is not going to be our problem. Because our smallest item is already going to be at the root of our tree. We just need to restore compliance to the heap structure.

So we remove our root, and then swap our next smallest item up to the root. Then we shuffle our tree to restore compliance. 


    7.12 Search Tree operations

We're gonna be going into binary search trees now.

Here are some operations:

Map() Create a new, empty map.
put(key,val) Add a new key-value pair to the map. If the key is already in the map then replace the old value with the new value.
get(key) Given a key, return the value stored in the map or None otherwise.
del Delete the key-value pair from the map using a statement of the form del map[key].
len() Return the number of key-value pairs stored in the map.
in Return True for a statement of the form key in map, if the given key is in the map.

    7.13 Search Tree Implementation

A search tree relies on that values that are less than the parent be found on the left side, and values that are more than the parent are found on the right. This is known as the bst property.

The yield keyword: when we call yield, we return a value but keep the state of the function. 

    7.14 Search Tree Analysis

The worst case performance for put in a balanced tree is O(log(n)), where n is the # of nodes in a tree.

We can create an inefficient tree by inserting our values in sorted order. This turns our performance into O(n).

    7.15 Balanced Binary trees

We mentioned before that unbalanced trees create bad search complexities, well now we're going to go over ways to balance out a tree. This is called an AVL tree, named after its inventors, and it automatically balances out its children.

To quantify if our tree is balanced, we can compute a balance factor. This is balancefactor = height(leftsubtree) - height(righsubtree). With our AVL tree, we compute this value node by node and if we breach a certain threshold then we rebalance the tree.

    7.16 AVL Tree performance

Our claim is that if we can have every node reach a balance factor of -1, 0, or 1 then we will have better performance than a regular search tree.

Our AVL search is log(n).

    7.17 AVL Tree Implementation

